% !TeX document-id = {f19fb972-db1f-447e-9d78-531139c30778}
% !BIB program = biber
\documentclass[compress]{beamer}
\usepackage[T1]{fontenc}
\usepackage{pifont}
\usetheme[block=fill,subsectionpage=progressbar,sectionpage=progressbar]{metropolis} 

\usepackage{wasysym}
\usepackage{etoolbox}
\usepackage[utf8]{inputenc}

\usepackage{threeparttable}
\usepackage{subcaption}

\usepackage{tikz-qtree}
\setbeamercovered{still covered={\opaqueness<1->{5}},again covered={\opaqueness<1->{100}}}


\usepackage{listings}

\lstset{
	basicstyle=\scriptsize\ttfamily,
	columns=flexible,
	breaklines=true,
	numbers=left,
	%stepsize=1,
	numberstyle=\tiny,
	backgroundcolor=\color[rgb]{0.85,0.90,1}
}



\lstnewenvironment{lstlistingoutput}{\lstset{basicstyle=\footnotesize\ttfamily,
		columns=flexible,
		breaklines=true,
		numbers=left,
		%stepsize=1,
		numberstyle=\tiny,
		backgroundcolor=\color[rgb]{.7,.7,.7}}}{}


\lstnewenvironment{lstlistingoutputtiny}{\lstset{basicstyle=\tiny\ttfamily,
		columns=flexible,
		breaklines=true,
		numbers=left,
		%stepsize=1,
		numberstyle=\tiny,
		backgroundcolor=\color[rgb]{.7,.7,.7}}}{}



\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[style=apa, backend = biber]{biblatex}
\DeclareLanguageMapping{american}{american-UoN}
\addbibresource{literature.bib}
\renewcommand*{\bibfont}{\tiny}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,matrix}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage{booktabs}
\usepackage{graphicx}

\graphicspath{{pictures/}}

\makeatletter
\setbeamertemplate{headline}{%
	\begin{beamercolorbox}[colsep=1.5pt]{upper separation line head}
	\end{beamercolorbox}
	\begin{beamercolorbox}{section in head/foot}
		\vskip2pt\insertnavigation{\paperwidth}\vskip2pt
	\end{beamercolorbox}%
	\begin{beamercolorbox}[colsep=1.5pt]{lower separation line head}
	\end{beamercolorbox}
}
\makeatother



\setbeamercolor{section in head/foot}{fg=normal text.bg, bg=structure.fg}



\newcommand{\question}[1]{
	\begin{frame}[plain]
		\begin{columns}
			\column{.3\textwidth}
			\makebox[\columnwidth]{
				\includegraphics[width=\columnwidth,height=\paperheight,keepaspectratio]{mannetje.png}}
			\column{.7\textwidth}
			\large
			\textcolor{orange}{\textbf{\emph{#1}}}
		\end{columns}
\end{frame}}


\title[ML in PYthon]{\textbf{COMPUTATIONAL OPINION ANALYSIS, COST ACTION CA21129 OPINION} \\14-06-2024, 12.30-13.30, Salamanca\\ »Machine Learning for Opinion Research«}
\author[Damian Trilling]{Damian Trilling \\ ~ \\ \footnotesize{d.c.trilling@uva.nl, @damian0604} \\}
\date{14-06-2024, 12.30-13.30}
\institute[VU]{Vrije Universiteit Amsterdam}


\begin{document}
	
	\begin{frame}{}
		\titlepage
	\end{frame}
	
	\begin{frame}{Today}
		\tableofcontents
	\end{frame}
	
	

\section[Top-down vs bottom-up]{Top-down vs bottom-up}


\begin{frame}[plain]
\makebox[\linewidth]{
\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{boumanstrilling2016}}
\cite{Boumans2016}
\pause

\textbf{\textcolor{orange}{The same logic applies to non-textual data!}}
\end{frame}
%}




\begin{frame}{Some terminology }
\begin{columns}[t]
\column{.5\textwidth}

\begin{block}<1-4>{Supervised machine learning}
You have a dataset with both predictor and outcome (independent and dependent variables; features and labels) --- a \emph{labeled} dataset.
\onslide<2>{
	\footnotesize{Think of regression: You measured \texttt{x1}, \texttt{x2}, \texttt{x3} and you want to predict \texttt{y}, which you also measured}}
\end{block}

\column{.5\textwidth}

\begin{block}<3->{Unsupervised machine learning}
You have no labels. \onslide<4>{(\footnotesize{You did not measure \texttt{y})}}\\
\onslide<5>{\textbf{Again, you already know some techniques to find out how \texttt{x1}, \texttt{x2},\ldots \texttt{x\_i} co-occur from other courses:} \begin{itemize}
		\item Principal Component Analysis (PCA) and Singular Value Decomposition (SVD)
		\item Cluster analysis
		\item Topic modelling (Latent Dirichlet Allocation)
		\item \ldots
	\end{itemize}
}
\end{block}

\end{columns}

\end{frame}


\begin{frame}{Unsupervised versus supervised methods in opinion research}
\begin{itemize}
	\item Valerie talked this morning in detail Topic Modeling (= unsupervised)
	\item Good to get an overview of of different topics/opinions/viewpoints/\ldots (it's complicated) in a large corpus
	\item Allows to discover topics one did not search for
\end{itemize}
\end{frame}


\begin{frame}{Unsupervised versus supervised methods in opinion research}
	\begin{itemize}
		\item But this does \textit{not} align well with automatically coding \textit{a-priori} or \textit{theoretically} defined concepts
		\item Example: ``pro-Russia'' vs ``pro-Ukraine'' vs ``neutral''
		\item Supervised methods 
	\end{itemize}
\end{frame}


\begin{frame}{Unsupervised versus supervised methods in opinion research}
\begin{alertblock}{Blurring boundaries}
More modern models (e.g., transformer models) are \textit{semi-supervised }: pre-trained on a large set of unlabeled data (unsupervised), fine-tuned on a smaller set of labeled data (supervised)
\end{alertblock}

\end{frame}





\section{Predicting things}






\subsection{You have done it before!}
\begin{frame}{You have done it before!}
	\begin{block}{Regression}<2->
		\begin{enumerate}
			\item<3-> Based on your data, you estimate some regression equation 	$y_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \varepsilon_i$
			\item<4-> Even if you have some \emph{new unseen data}, you can estimate your expected outcome $\hat{y}$!
			\item<5-> Example: You estimated a regression equation where $y$ is newspaper reading in days/week: $y = -.8 + .4 \times man + .08 \times age$
			\item<6-> You could now calculate $\hat{y}$ for a man of 20 years and a woman of 40 years -- \emph{even if no such person exists in your dataset}: \\
			$\hat{y}_{man20} = -.8 + .4 \times 1 + .08 \times 20 = 1.2$ \\
			$\hat{y}_{woman40} = -.8 + .4 \times 0 + .08 \times 40 = 2.4$
		\end{enumerate}
	\end{block}	
	
\end{frame}



\begin{frame}{}
	\huge{This is\\ Supervised Machine Learning!}
\end{frame}

\begin{frame}{\ldots but\ldots}
	\begin{itemize}
		\item<1-> We will only use \emph{half} {\tiny{(or another fraction)}} of our data to estimate the model, so that we can use the other half to check if our predictions match the manual coding (``labeled data'',``annotated data'' in SML-lingo)
		\begin{itemize}
			\item<2->e.g., 2000 labeled cases, 1000 for training, 1000 for testing --- if successful, run on 100,000 unlabeled cases
		\end{itemize}
		\item<3-> We use many more independent variables (``features'')
		\item<4-> Typically, IVs are word frequencies (often weighted, e.g. tf$\times$idf) ($\Rightarrow$BOW-representation)
	\end{itemize}
\end{frame}


\subsection{From regression to classification}
	
\begin{frame}[standout]
In the machine learning world, predicting some continous value is referred to as a \textcolor{orange}{regression} task. If we want to predict a binary or categorical variable, we call it a \textcolor{orange}{classification} task.

(quite confusingly, even if we use a logistic regression for the latter)
\end{frame}


\begin{frame}{Classification tasks}
For many computational approaches, we are actually not that interested in predicting a continous value. Typical questions include:
\begin{itemize}
	\item Is this article about topix A, B, C, D, or E?
	\item Is this review positive or negative?
	\item Does this text contain frame F?
	\item I this satire? 
	\item Is this misinformation?
	\item Given past behavior, can I predict the next click?
\end{itemize}
\end{frame}



\begin{frame}[plain]
	\begin{columns}[]
		\column{.5\textwidth}
		
		{\tiny{http://commons.wikimedia.org/wiki/File:Precisionrecall.svg}}
		\makebox[\linewidth]{
			\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{../pictures/precisionrecall.png}}
		
		\column{.5\textwidth}
		\begin{block}{Some measures}
			\begin{itemize}
				\item Accuracy
				\item Recall
				\item Precision
				\item $\text{F1}=2\cdot \frac{\text{precision}\cdot \text{recall}}{\text{precision}+\text{recall}}$
				\item AUC (Area under curve) $[0,1]$, $0.5=$ random guessing
			\end{itemize}
		\end{block}
		
		
	\end{columns}
	
\end{frame}



\begin{frame}[standout]
	\huge{More details this afternoon!}
\end{frame}




\section{Machine Learning for Opinions}


\subsection{Conceptual clarifications}

\begin{frame}{Beyond sentiment}
	\begin{block}{A lot of -- partly contradictory -- definitions and operationalizations}
		\begin{itemize}
			\item ``sentiment'' or ``sentiment analyis'' often used as overarching term
			\item valence/polarity: (how) positive vs (how) negative
			\item stance detection: stance \textit{towards}
			\item aspect-based sentiment analysis
			\item \ldots
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[standout]
	\large
	\textcolor{red}{Unemployment} is going \textcolor{red}{down}.
\end{frame}


\begin{frame}[standout]
	\large
	Trump \textcolor{green}{supported} the law, while Biden \textcolor{red}{opposed} it.\\
	Biden \textcolor{green}{supported} the law, while Trump \textcolor{red}{opposed} it.\\
	Biden \textcolor{green}{supported} Trump's decision to \textcolor{red}{oppose} the law.
\end{frame}



\begin{frame}{Beyond sentiment}
	
\textbf{An opinion should have at least a target (what is evaluated?), and maybe also a source (who evaluates?)}
\end{frame}

\begin{frame}{Can we do anything at all?}
	\begin{itemize}
		\item It depends on the complexity of the task
		\item Maybe errors cancel each other out (?)
		\item In any case: \textbf{Need for careful evaluation}
		
	\end{itemize}
\end{frame}



\subsection{(Traditional)) non-SML approaches}

\begin{frame}{Let's consider three tasks}
	For a given text (say, a news article, a press release, a review), determine the
	\begin{description}
		\item[sentiment] e.g., [positive|neutral|negative]
		\item[frames] e.g., [economic|human|moral|conflict], or non-exclusive: economic = [0|1], human = [0|1], \ldots
		\item[related concepts] e.g., perspectives, viewpoints, etc.
	\end{description}
\end{frame}


\question{What would be the strengths and weaknesses of different approaches for each of these tasks?}


\question{Imagine using a dictionary-based (list of keywords, list of regular expressions, or similar) approach to these tasks. How does the design (length, inclusiveness, etc.) of this list influence precision and recall?}


\begin{frame}{Dictionary-based approaches for text classification}
	\begin{columns}[t]
		\column{.5\textwidth}
		\begin{block}{good for}
			\begin{itemize}
				\item distinct, manifest things (names of organizations, pronouns, swearwords (?), \ldots)
				\item little room for interpretation/misunderstandings etc.
				\item ``must-be-explainable-to-a-five-year-old''
			\end{itemize}
			\pause
		\end{block}
		\column{.5\textwidth}
		\begin{alertblock}{bad for}
			\begin{itemize}
				\item latent constructs and concepts
				\item implicit things
			\end{itemize}
			\pause
			Hence, \emph{not} state-of-the-art for
			\begin{itemize}
				\item topics
				\item frames
				\item sentiment
			\end{itemize}
		\end{alertblock}
	\end{columns}
\end{frame}


\begin{frame}{From dictionary approaches to SML}
	\begin{itemize}[<+->]
		\item Early days of sentiment analysis: list of positive words, list of negative words, count what occurs most
		\item You can even \textit{buy} lists of words that are meant to measure constructs like ``positive emotions'' or even ``analytic'' or ``authentic'' language use from a psychologist (LIWC, \cite{Pennebaker2007})
	\end{itemize}
\end{frame}

\question{What do you think? Can this even work}





\begin{frame}{Bag-of-words dictionary approaches to sentiment analysis}
	\begin{block}{con}
		\begin{itemize}
			\item simplistic assumptions
			\item e.g., intensifiers cannot be interpreted (``really'' in ``really good'' or ``really bad'')
			\item or, even more important, negations.
		\end{itemize}
	\end{block}
\end{frame}




\begin{frame}{Improving the BOW approach}
	\begin{block}{Example: Sentistrenght \parencite{Thelwall2012}}
		\begin{itemize}
			\item $-5\ldots-1$ and $+1\ldots+5$ instead of positive/negative
			\item spelling correction
			\item ``booster word list'' for strengthening/weakening the effect of the following word
			\item interpreting repeated letters (``baaaaaad''), CAPITALS and !!!
			\item idioms
			\item negation 
		\end{itemize}
	\end{block}
	
	VADER by \cite{Hutto2014} works in a similar way.
	\pause
	\footnotesize{Even though this is much less na\"ive than LIWC, for instance, the problem remains: Can we construct a dictionary that, \emph{irrespective of the context}, gives us a meaningful estimate of sentiment? }
\end{frame}





\begin{frame}[standout]
	Such an \textit{off-the-shelf} dictionary does not (and probably cannot) exist.
\end{frame}



\begin{frame}{\cite{Boukes2020}: Sentiment analysis of economic news}
	\makebox[\columnwidth]{
		\includegraphics[width=\columnwidth,height=.8\paperheight,keepaspectratio]{boukes2019}}
\end{frame}


\begin{frame}{\cite{Boukes2020}: Sentiment analysis of economic news}
	\makebox[\columnwidth]{
		\includegraphics[width=\columnwidth,height=.8\paperheight,keepaspectratio]{boukes2019b}}
\end{frame}


\begin{frame}{\cite{Boukes2020}: Sentiment analysis of economic news}
	\begin{itemize}
		\item Dictionaries have low agreement with each other, and also with human coders
		\item Even their own dictionary didn't agree
		\item \textbf{This is not because these dictionaries are particularly bad!}. Main point: For such a complex and context-dependent task, a dictionary is just not the right tool.
	\end{itemize}
\end{frame}




\begin{frame}{\cite{VanAtteveldt2021}: Extending \cite{Boukes2020} with SML}
	``manual coding (using undergraduate students) yields the
	best results 
	
	[\ldots] A good second place is taken by crowd coding [\ldots]  
	
	
	[\ldots] machine learning performs worse than both students' manual coding and crowd coding.
	Reaching $\alpha = 0.50$ for deep learning (CNN) and slightly worse for classical machine learning (SVM; $\alpha = 0.41$, NB; $\alpha = 0.40$), machine learning still performs significantly better than chance. However, since these results are lower than generally accepted levels of inter-coder reliability [\ldots]
	
	Finally, [\ldots] dictionaries [\ldots] perform worse than the machine
	learning results and much worse than manual annotation [\ldots] [and] approximate chance agreement''\end{frame}






\begin{frame}{\cite{Vermeer2019}: Satisfaction with brands}
	\makebox[\columnwidth]{
		\includegraphics[width=\columnwidth,height=.8\paperheight,keepaspectratio]{vermeer2019}}
\end{frame}



\begin{frame}[standout]
	SML is no panacea, but the most promising approach to analyzing large quantities of texts. Don't believe off-the-shelf packages that claim to do the work for you.
	(For small datasets, just do it by hand.)
\end{frame}






\begin{frame}{What does this mean for our research?}
	\begin{block}<2>{It we have (say) 2,000 documents with manually coded kabels\ldots}
		\begin{itemize}
			\item we can use them to train a SML classifier
			\item which can code an unlimited number of new documents
			\item with an acceptable accuracy (at least for some of them)
		\end{itemize}
	\end{block}

\end{frame}



\subsection{An implementation}

\begin{frame}[fragile]{An implementation}
Let's say we have two list of tuples with movie reviews and their rating:
\begin{lstlisting}
reviews_train = ["This is a great movie", "Bad movie", ... ...]
labels_train = [1,-1, ...]  
\end{lstlisting}
And a second dataset with an identical structure:
\begin{lstlisting}
reviews_test = ["Not that good","Nice film", ... ...]
labels_text = [-1,1, ......]
\end{lstlisting}
Both are drawn from the same population, it is pure chance whether a specific review is on the one list or the other.\\
\tiny{Based on an example from \url{http://blog.dataquest.io/blog/naive-bayes-movies/}}
\end{frame}

\begin{frame}[fragile]{Training a A Naïve Bayes Classifier}
  \begin{lstlisting}
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics

vectorizer = CountVectorizer(stop_words='english')
features_train = vectorizer.fit_transform(reviews_train)
features_test = vectorizer.transform(reviews_test)

# Fit a naive bayes model to the training data.
nb = MultinomialNB()
nb.fit(features_train, labels_train)

# Now we can use the model to predict classifications for our test features.
predictions = nb.predict(features_test)

print(f"Precision:\t{metrics.precision_score(labels_test, predictions, pos_label=1, labels = [-1,1]))}"
print(f"Recall:\t{metrics.recall_score(labels_test, predictions, pos_label=1, labels = [-1,1]))}"
\end{lstlisting}
\end{frame}
%
%\begin{frame}{TODO}
%TODO\\
%andere vectorizer (TFIDF)\\
%verschillen classifiers\\
%andere output (metrics summary)
%\\
%waarom is dit hier een MULTINOMIAL NB
%\\
%scikit-learn installeren\\ 
%opdracht bedenken: classifiers vergelijken
%\end{frame}


\begin{frame}{And it works!}
	Using 50,000 IMDB movies that are classified as either negative or positive,
	\begin{itemize}
		\item I created a list with 25,000 training tuples and another one with 25,000 test tuples and
		\item trained a classifier
		\item with precision and recall values $>.80$
	\end{itemize}
	~\\
	\tiny{Dataset obtained from \url{http://ai.stanford.edu/~amaas/data/sentiment}, Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., \& Potts, C. (2011). Learning word vectors for sentiment analysis. \emph{49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)}
	}
	
\end{frame}

\begin{frame}[fragile]{Playing around with new data}
\begin{lstlisting}
newdata=vectorizer.transform(["What a crappy movie! It sucks!", "This is awsome. I liked this movie a lot, fantastic actors","I would not recomment it to anyone.", "Enjoyed it a lot"])
predictions = nb.predict(newdata)
print(predictions)
\end{lstlisting}
This returns, as you would expect and hope:
\begin{lstlisting} 
[-1  1 -1  1]
\end{lstlisting}


\end{frame}


\question{Can you relate the IMDB-example to our earlier discussion on sentiment analysis? Why does ML work so well here?}



\begin{frame}{But we can do even better}
	We can use different vectorizers and different classifiers.
\end{frame}


\subsection{Classifiers}


\begin{frame}{Different classifiers}
	Typical options in a nutshell:
	\begin{itemize}
		\item Na\"ive Bayes
		\item Logistic Regression
		\item Support Vector Machine (SVM/SVC)
		\item Random forests
	\end{itemize}
\end{frame}


\subsection{Vectorizers}

\begin{frame}{Different vectorizers}
	\begin{enumerate}[<+->]
		\item CountVectorizer (=simple word counts)
		\item TfidfVectorizer (word counts (``term frequency'') weighted by number of documents in which the word occurs at all (``inverse document frequency''))
	\end{enumerate}
	
	\pause
	$$tfidf_{t,d} = tf_{t,d} \cdot idf_{t}$$
	
	There are different ways to weigh the idf score. A common one is taking the logarithm:
	
	$$idf_{t} = \log \frac{N}{n_t}$$
	
	where $N$ is the total number of documents and $n_t$ is the number of documents containing term $t$
\end{frame}

\begin{frame}{Different vectorizer options}
	\begin{itemize}
		\item Preprocessing (e.g., stopword removal)
		\item Remove words below a specific threshold (``occurring in less than $n=5$ documents'') $\Rightarrow$ spelling mistakes etc.
		\item Remove words above a specific threshold (``occuring in more than 50\% of all documents) $\Rightarrow$ de-facto stopwords
		\item Not only to improve prediction, but also performance (can reduce number of features by a huge amount)
	\end{itemize}
\end{frame}








\begin{frame}{Which one would you (not) use for which purpose?}
	
	\makebox[\linewidth]{
		\includegraphics[width=\paperwidth,height=.8\paperheight,keepaspectratio]{imdbsml}}
\end{frame}




\section{Summing up}

\subsection{Revisiting the difference between the dictionary approach and the SML}


\begin{frame}{What \emph{is} our fitted classifier again?}
	Essentially, just a formula 
	
	$$p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}}$$
	
	where $\beta_0$ is an intercept\footnote{Machine Learning people sometimes call the intercept ``bias'' (yes, I know, that's confusing)}, $\beta_1$ a coefficient for the frequency (or tf$\cdot$ idf score) of some word, $\beta_2$ a coefficient some other word.
	
	If our fitted \emph{vectorizer} contains 5,000 words, we thus have 5,001 coefficients.
	
	\tiny{(for logistic regression in this case, but same argument applies to other classifiers as well)}
	
\end{frame}

\question{But isn't that then essentially very much like a dictionary, except that the words have different weights?}


\begin{frame}{In some sense, yes.}
	\begin{itemize}
		\item But we don't pretend that we can construct the dictionary \emph{a priori}.
		\item It's specifically tailored to our use-case.
		\item The weights are \emph{really} essential here.
	\end{itemize}
	
	\pause
	We \emph{could} print all coefficients-word pairs, but probably it's enough to just look at those with the largest absolute value:
\end{frame}





\begin{frame}{Feature weights}
	\makebox[\linewidth]{
		\includegraphics[width=\paperwidth,height=.8\paperheight,keepaspectratio]{eli5}}
\end{frame}

\begin{frame}{ELI5}
	\begin{itemize}
		\item Inspecting \emph{all} coefficients of a ML model usually doesn't make much sense
		\item But that does not mean that we cannot understand how the model makes its predictions
		\item We can look at the most important coefficients
		\item We can look which words in a given text contributed most to its classfication
	\end{itemize}
\end{frame}




\begin{frame}{But have we solved all problems of dictionaries?}
	No.
	
	For instance, the negation and/or intensifier problem.
	
	Possible approaches
	\begin{itemize}
		\item $n$-grams as features
		\item preprocessing (?)
		\item deep learning 
		\item \ldots
	\end{itemize}
	\pause
	
	$\Rightarrow$ \textbf{But ultimately, it's just an empirical question how big the problem is!}
	
	
\end{frame}












\section{Summing up \& Looking forward}

\begin{frame}{A note on contemporary approaches}
\begin{block}{Advantages of Conventional/classic machine learning }
	\begin{itemize}
		\item Easy to do ($\Rightarrow$ this afternoon)
		\item many models are comparatively easy to understand and transparent: e.g., regression coefficients linking word frequencies to probability of a label
		\item no (compuationally, financially, environmentally) expensive researches needed
		\item (still) very useful as a baseline
	\end{itemize}
\end{block}
\end{frame}

	
\begin{frame}{A note on contemporary approaches}
	\begin{alertblock}{However,\ldots}
		\begin{itemize}
			\item Classic SML models are ``dumb'': they do not know anything about what the words mean
			\item hence, if a word is not in the training data, it remains unknown to the model and will be ignored
			\item BOW: no taking into account of sentence structure
		\end{itemize}
	\end{alertblock}
\end{frame}



\begin{frame}{A note on contemporary approaches}
	\begin{block}{Alternartives}
		\begin{itemize}
			\item Deep learning, neural networks, \ldots: Different architectures, e.g. to model ``latent'' constructs or to take sentence structure into account
			\item But: Even those (party) superseeded by transformer models (from BERT to GPT)
			\item Pretrained model with ``knowledge'' about language plus finetuning, few-shot, zero-shot learning 
		\end{itemize}
	\end{block}
\onslide<1>{$\Rightarrow$ Ultimately, you may use more advanced approaches -- but always compare to simpler baselines}	
\end{frame}



\question{Any questions?}

\begin{frame}{Things to remember}
\begin{itemize}
	\item unsupervised vs supervised
	\item evaluation metrics (e.g., precision, recall)
	\item why ``sentiment'' often is a problematic concept 
\end{itemize}
\end{frame}


\begin{frame}[plain]
	\printbibliography
\end{frame}


\end{document}
